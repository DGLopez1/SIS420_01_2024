{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/axr/blob/master/axr/00_intro.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de aplicación: tres en raya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Con el objetivo de ilustrar la idea general del axr vamos a considerar un ejemplo en detalle: el juego del tres en raya. En este juego, dos jugadores se turnan para dibujar una X o una O en un tablero con 3x3 posiciones. El primer jugador en conseguir dibujar tres figuras en una línea horizontal, vertical o diagonal, gana. Nuestro objetivo es conseguir un agente que sea capaz de ganar siempre a este juego.\n",
    "\n",
    "![](https://camo.githubusercontent.com/9b5f16d0451a8b6faf1ec45a6afbe22f55bbe1f9/68747470733a2f2f7468756d62732e6766796361742e636f6d2f506f697365644772697070696e67466f782d736d616c6c2e676966)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver este juego con axr, en primer lugar definimos una tabla de números, uno por cada posible estado del juego. Cada numero respresentará la probabilidad de ganar el juego desde ese estado, el *valor* del estado. Así pues, la tabla sería la *función de valor*. Un estado $A$ es considerado mejor que un estado $B$ si el valor estimado de la probabilidad de ganar el juego desde $A$ es mayor que desde $B$. Si jugásemos con las Xs, todos los estados con tres X en raya tenría un valor de 1, ya que hemos ganado el juego. De la misma manera, cualquier estado con tres Os en raya tendría un valor de 0, hemos perdido. Para la inicialización de la tabla, podemos establecer el resto de valores en 0.5 (50% de posibilidades de ganar).\n",
    "\n",
    "Nuestro agente jugará muchas partidas contra un oponente (que puede ser otro agente). En cada turno evaluamos los estados que resultarían de cada posible movimiento (posiciones no ocupadas) y elegimos aquella con un mayor *valor*. Ocasionalmente, elegiremos una acción aleatoria con el objetivo de explorar nuevos movimientos.\n",
    "        \n",
    "Mientras el agente va jugando, tendremos que actualizar la función de valor. Para ello, después de cada movimiento, cambiaremos el valor del estado del que venimos para que se acerque al valor del estado actual.\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha [V(S_{t+1}) - V(S_t)]\n",
    "$$\n",
    "      \n",
    "donde $S_t$ denote el estado del que venimos, $S_{t+1}$ es el nuevo estado después del movimiento, $V(S_t)$ es el valor del estado $S_t$ y $\\alpha$ es el ratio de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Board():\n",
    "    def __init__(self):\n",
    "        self.state = np.zeros((3,3))\n",
    "\n",
    "    def valid_moves(self):\n",
    "        return [(i, j) for j in range(3) for i in range(3) if self.state[i, j] == 0]\n",
    "\n",
    "    def update(self, symbol, row, col):\n",
    "        if self.state[row, col] == 0:\n",
    "            self.state[row, col] = symbol\n",
    "        else:\n",
    "            raise ValueError (\"movimiento ilegal !\")\n",
    "\n",
    "    def is_game_over(self):\n",
    "        # comprobar filas y columnas\n",
    "        if (self.state.sum(axis=0) == 3).sum() >= 1 or (self.state.sum(axis=1) == 3).sum() >= 1:\n",
    "            return 1\n",
    "        if (self.state.sum(axis=0) == -3).sum() >= 1 or (self.state.sum(axis=1) == -3).sum() >= 1:\n",
    "            return -1 \n",
    "        # comprobar diagonales\n",
    "        diag_sums = [\n",
    "            sum([self.state[i, i] for i in range(3)]),\n",
    "            sum([self.state[i, 3 - i - 1] for i in range(3)]),\n",
    "        ]\n",
    "        if diag_sums[0] == 3 or diag_sums[1] == 3:\n",
    "            return 1\n",
    "        if diag_sums[0] == -3 or diag_sums[1] == -3:\n",
    "            return -1        \n",
    "        # empate\n",
    "        if len(self.valid_moves()) == 0:\n",
    "            return 0\n",
    "        # seguir jugando\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros((3,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "class Game():\n",
    "    def __init__(self, player1, player2):\n",
    "        player1.symbol = 1\n",
    "        player2.symbol = -1\n",
    "        self.players = [player1, player2]\n",
    "        self.board = Board()\n",
    "\n",
    "    def selfplay(self, rounds=100):\n",
    "        wins = [0, 0]\n",
    "        for i in tqdm(range(1, rounds + 1)):\n",
    "            self.board.reset()\n",
    "            for player in self.players:\n",
    "                player.reset()\n",
    "            game_over = False\n",
    "            while not game_over:\n",
    "                for player in self.players:\n",
    "                    action = player.move(self.board)\n",
    "                    self.board.update(player.symbol, action[0], action[1])\n",
    "                    for player in self.players:\n",
    "                        player.update(self.board)\n",
    "                    if self.board.is_game_over() is not None:\n",
    "                        game_over = True\n",
    "                        break\n",
    "            self.reward()\n",
    "            for ix, player in enumerate(self.players):\n",
    "                if self.board.is_game_over() == player.symbol: \n",
    "                    wins[ix] += 1\n",
    "        return wins\n",
    "\n",
    "\n",
    "    def reward(self):\n",
    "        winner = self.board.is_game_over()\n",
    "        if winner == 0: # empate\n",
    "            for player in self.players:\n",
    "                player.reward(0.5)\n",
    "        else: # le damos 1 recompensa al jugador que gana\n",
    "            for player in self.players:\n",
    "                if winner == player.symbol:\n",
    "                    player.reward(1)\n",
    "                else:\n",
    "                    player.reward(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, alpha=0.5, prob_exp=0.5):\n",
    "        self.value_function = {} # tabla con pares estado -> valor\n",
    "        self.alpha = alpha         # learning rate\n",
    "        self.positions = []       # guardamos todas las posiciones de la partida\n",
    "        self.prob_exp = prob_exp   # probabilidad de explorar\n",
    "\n",
    "    def reset(self):\n",
    "        self.positions = []\n",
    "\n",
    "    def move(self, board, explore=True):\n",
    "        valid_moves = board.valid_moves()\n",
    "        # exploracion\n",
    "        if explore and np.random.uniform(0, 1) < self.prob_exp:\n",
    "            # vamos a una posición aleatoria\n",
    "            ix = np.random.choice(len(valid_moves))\n",
    "            return valid_moves[ix]\n",
    "        # explotacion\n",
    "        # vamos a la posición con más valor\n",
    "        max_value = -1000\n",
    "        for row, col in valid_moves:\n",
    "            next_board = board.state.copy()\n",
    "            next_board[row, col] = self.symbol\n",
    "            next_state = str(next_board.reshape(3*3))\n",
    "            value = 0 if self.value_function.get(next_state) is None else self.value_function.get(next_state)\n",
    "            if value >= max_value:\n",
    "                max_value = value\n",
    "                best_row, best_col = row, col\n",
    "        return best_row, best_col\n",
    "\n",
    "    def update(self, board):\n",
    "        self.positions.append(str(board.state.reshape(3*3)))\n",
    "\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # al final de la partida (cuando recibimos la recompensa)\n",
    "        # iteramos por tods los estados actualizando su valor en la tabla\n",
    "        for p in reversed(self.positions):\n",
    "            if self.value_function.get(p) is None:\n",
    "                self.value_function[p] = 0\n",
    "            self.value_function[p] += self.alpha * (reward - self.value_function[p])\n",
    "            reward = self.value_function[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:48<00:00, 207.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5615, 2750]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1 = Agent(prob_exp=0.5)\n",
    "agent2 = Agent()\n",
    "\n",
    "game = Game(agent1, agent2)\n",
    "\n",
    "game.selfplay(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estado</th>\n",
       "      <th>valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ 1.  1. -1.  1. -1. -1.  1. -1.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1.  0.  1.  0. -1.  1.  0.  0.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 1.  1.  1.  1. -1. -1. -1. -1.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ 1.  0.  0. -1.  1.  0. -1.  0.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 0. -1.  0.  0. -1.  0.  1.  1.  1.]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4706</th>\n",
       "      <td>[ 1.  1.  0.  0.  1. -1.  1. -1. -1.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>[-1.  1.  1.  1.  0.  0. -1. -1.  0.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4708</th>\n",
       "      <td>[ 1.  1. -1.  0. -1.  1.  0. -1.  0.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>[-1.  1.  0. -1.  0.  1. -1.  1.  0.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4710</th>\n",
       "      <td>[ 1.  0.  1.  0.  0.  1. -1. -1.  0.]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4711 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     estado  valor\n",
       "0     [ 1.  1. -1.  1. -1. -1.  1. -1.  1.]    1.0\n",
       "1     [-1.  0.  1.  0. -1.  1.  0.  0.  1.]    1.0\n",
       "2     [ 1.  1.  1.  1. -1. -1. -1. -1.  1.]    1.0\n",
       "3     [ 1.  0.  0. -1.  1.  0. -1.  0.  1.]    1.0\n",
       "4     [ 0. -1.  0.  0. -1.  0.  1.  1.  1.]    1.0\n",
       "...                                     ...    ...\n",
       "4706  [ 1.  1.  0.  0.  1. -1.  1. -1. -1.]    0.0\n",
       "4707  [-1.  1.  1.  1.  0.  0. -1. -1.  0.]    0.0\n",
       "4708  [ 1.  1. -1.  0. -1.  1.  0. -1.  0.]    0.0\n",
       "4709  [-1.  1.  0. -1.  0.  1. -1.  1.  0.]    0.0\n",
       "4710  [ 1.  0.  1.  0.  0.  1. -1. -1.  0.]    0.0\n",
       "\n",
       "[4711 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "funcion_de_valor = sorted(agent1.value_function.items(), key=lambda kv: kv[1], reverse=True)\n",
    "tabla = pd.DataFrame({'estado': [x[0] for x in funcion_de_valor], 'valor': [x[1] for x in funcion_de_valor]})\n",
    "\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('agente.pickle', 'wb') as handle:\n",
    "    pickle.dump(agent1.value_function, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo sirve para ilustrar algunas de las propiedades clave del axr. En primer lugar, aprender a través de la interacción con el entorno (en este caso el otro agente). En segundo lugar, tenemos un objetivo claro y el comportamiento correcto del agente requiere de planificación y predicción que tenga en cuenta los efectos futuros de sus acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo es una aproximación computacional a la comprensión y automatización del aprendizaje por objetivos y toma de decisiones. En esta aproximación, una agente aprende a través de la interacción directa con su entorno sin necesidad de supervisión explícita. Utiliza procesos de decisión de Markov para definir la interacción entre el agente y su entorno en términos de estados, acciones y recompensas. Los conceptos de valor y función de valor son la clave de muchos métodos de axr ya que representan una manera eficiente de búsqueda en el espacio de políticas."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb9f406c0f70fca9801e60f2cbb7cd1ccff2ae2f74c58f513340bcf6cae5ecd0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
